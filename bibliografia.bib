%% PPGCC - CCN - UFPI

%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/
%% Exemplo de arquivo .bib a ser utilizado para gerar a Biliografia

%% Quando for no Google Scholar, clique em bibtex na citação e copie o código pra colar aqui

@article{Sardim2020,
   abstract = {RESUMO O objetivo do estudo foi avaliar o efeito da fotobiomodulação associada a exercícios na dor e na funcionalidade de pacientes com osteoartrite de joelho. Para isso foram recrutados 20 pacientes com osteoartrite do joelho uni ou bilateral, que foram distribuídos em dois grupos: grupo-controle (GC), que realizou aplicação de fotobiomodulação (FBM) placebo e um protocolo de exercício; e grupo fotobiomodulação (GF), que realizou aplicação ativa da FBM e o protocolo de exercício, sendo esse realizado duas vezes por semana durante oito semanas e consistindo de alongamentos passivos dos músculos de membros inferiores, straight leg raise, treinamento proprioceptivo e exercícios para o controle da marcha. A FBM foi aplicada com o aparelho cluster contendo quatro diodos de 670 nm e cinco diodos de 850 nm, com uma potência de saída de 540 mW, sendo a dose utilizada de 4 J/cm2. Os grupos foram avaliados pré e pós-tratamento com os questionários SF-36, Lequesne, Tinetti, e por meio da Escala Visual Analógica de dor (EVA). Os dados foram analisados com o método Anova, seguido do Bonferroni. Os dados indicaram melhoras significativas para o GF ao fim do tratamento para as avaliações da EVA (2±1,25 vs. 0,7±0,82; p=0,009). Embora ambos os grupos tenham obtido melhoras significativas ao longo do tratamento, não foi possível observar diferenças significativas entre eles para o restante das avaliações ao final do tratamento. Portanto, conclui-se que o uso da FBM associada a exercícios apresentou melhora da dor nos pacientes com osteoartrite de joelho, embora não tenha sido possível observar diferenças significativas no que diz respeito à funcionalidade.ABSTRACT Our study evaluated the effect of photobiomodulation associated with exercise on pain and functionality of patients with knee osteoarthritis. Twenty patients with unilateral or bilateral knee osteoarthritis were selected and divided into two groups: Control group (CG), which underwent photobiomodulation (PBM) placebo and exercise protocol; and the photobiomodulation group (PG), which performed an active application of the PBM and the exercise protocol - performed twice a week for eight weeks and consisting of passive stretching of the lower extremity muscles, straight leg raise, proprioceptive training and exercises for gait control. PBM was applied through a cluster apparatus containing four diodes of 670 nm and five diodes of 850 nm, with an output power of 540 mW and with a dose of 4J / cm2. The groups were evaluated before and after treatment with the SF-36, Lequesne and Tinetti questionnaires as well as the Visual Analog Pain Scale. Data were analyzed using the Anova method, followed by the post-hoc Bonferroni test. The data indicated significant improvements in the PG at the end of treatment for Visual Analog Pain Scale (2±1.25 vs. 0.7±0.82, p=0.009). Although both groups achieved significant improvements throughout the treatment, we could not observe significant differences between them for the rest of the evaluations at the end of the treatment. Therefore, the use of PBM associated with exercises showed pain improvement in patients with knee osteoarthritis, although it was not possible to observe significant differences in patients’ functionality.RESUMEN El presente estudio tuvo como objetivo evaluar el efecto de la fotobiomodulación asociada a ejercicios sobre el dolor y la funcionalidad de pacientes con osteoartritis de rodilla. Para ello, se reclutaron a 20 pacientes con osteoartritis de rodilla unilateral o bilateral, que se dividieron en dos grupos: el grupo control (GC), que recibió placebo de fotobiomodulación (FBM) y un protocolo de ejercicio; y el grupo de fotobiomodulación (GF), que recibió la aplicación activa de FBM y el protocolo de ejercicio, el cual se realizó dos veces por semana, durante ocho semanas, y consistió en estiramientos pasivos de los músculos de las extremidades inferiores, straight leg raise, entrenamiento propioceptivo y ejercicios para el control de la marcha. La FBM se aplicó con el dispositivo cluster que contenía cuatro diodos de 670 nm y cinco diodos de 850 nm, con una potencia de salida de 540 mW, y la dosis utilizada fue de 4 J/cm2. Los grupos se evaluaron antes y después del tratamiento por medio de los cuestionarios SF-36, Lequesne, Tinetti y de la Escala Visual Analógica de Dolor (EVA). Los datos se analizaron utilizando el método Anova, seguido del Bonferroni. Los datos apuntaron una mejora significativa en el GF al final del tratamiento mediante las evaluaciones de la EVA (2±1,25 vs. 0,7±0,82; p=0,009). A pesar de que ambos grupos lograron obtener mejoras significativas durante el curso del tratamiento, no fue posible observar diferencias significativas entre ellos en las evaluaciones al final del tratamiento. Por lo tanto, se concluye que el uso de la FBM asociada a ejercicios ocasionó una mejora del dolor en pacientes con osteoartritis de rodilla, aunque no fue posible observar diferencias significativas con respecto a la funcionalidad.},
   author = {André Cabral Sardim and Rodrigo Paschoal Prado and Carlos Eduardo Pinfildi},
   doi = {10.1590/1809-2950/18020027022020},
   issn = {1809-2950},
   issue = {2},
   journal = {Fisioterapia e Pesquisa},
   title = {Efeito da fotobiomodulação associada a exercícios na dor e na funcionalidade de pacientes com osteoartrite de joelho: estudo-piloto},
   volume = {27},
   year = {2020},
}
@article{PACCA2018,
   abstract = {ABSTRACT Background: High body mass index, as well as maintaining this condition for a long period of time, are important risk factors for the development of osteoarthritis. Aim: To determine joint pain and osteoarthritis prevalence in patients referred to bariatric surgery. Methods: Morbidly obese patients referred to bariatric surgery responded to the visual analogue pain scale (VAS) and the WOMAC questionnaire. X-rays of the hips and knees were evaluated. The primary endpoints were self-reported joint pain and the diagnosis of osteoarthritis by clinical and radiological criteria of the American College of Rheumatology. Results: 141 patients were interviewed (85.1% women) with a mean age of 40 years. The mean body mass index was 46. The lumbar spine and knee joint were the most commonly reported as painful (77.9% and 73.2% respectively). Prevalence of knee osteoarthritis was 63.1% and hip osteoarthritis was 40.8%. Age, mean VAS and WOMAC scores were higher in the osteoarthritic individuals. Conclusion: There is prevalence of 90.1% of pain symptoms in morbidly obese patients referred to bariatric surgery. The prevalence of knee osteoarthritis was 63.1% and hip osteoarthritis was 40.8% in this sample.},
   author = {Daniel Moreira PACCA and Gustavo Constantino DE-CAMPOS and Alessandro Rozin ZORZI and Elinton Adami CHAIM and Jõao Batista DE-MIRANDA},
   doi = {10.1590/0102-672020180001e1344},
   issn = {2317-6326},
   issue = {1},
   journal = {ABCD. Arquivos Brasileiros de Cirurgia Digestiva (São Paulo)},
   title = {Prevalência de dor articular e osteoartrite na população obesa brasileira},
   volume = {31},
   year = {2018},
}
@article{Luis2022,
   abstract = {INTRODUÇÃO: a osteoartrite do joelho (OA) é uma doença articular degenerativa, que ocasiona desgaste e perda progressiva da cartilagem local. Os sintomas de OA incluem rigidez, mobilidade articular limitada e presença de dor que podem levar a uma diminuição na qualidade de vida e comprometimento da saúde. A interpretação/análise clínica de imagens suporta um trabalho cada vez maior nos centros ortopédicos e de radiologia. O presente estudo desenvolveu e aplicou o desempenho de uma rede neural convolucional projetada para auxiliar ortopedistas e radiologistas na detecção e classificação de osteoartrite do joelho de graus iniciais a severos, de acordo com o sistema de classificação Kellgren-Lawrence (KL). OBJETIVO: Desenvolver e validar uma CNN capaz de classificar e diagnosticar a OA no joelho de forma dinâmica e eficaz. Material e métodos: Utilizou-se uma pesquisa descritiva de caráter quali quantitativo e métodos da IA aplicados na análise do movimento humano. Foi utilizado um banco de dados com radiografias de OA de joelho (Grau 0 - 3.085 imagens, Grau 1 - 1.416 imagens, Grau 2 - 2.062 imagens, Grau 3 - 1.029 imagens e Grau 4 - 236 imagens). Para análise e classificação das imagens foi utilizado um ambiente de desenvolvimento da linguagem Python, por meio da aplicação Google Colab executada via browser. As imagens foram utilizadas para treinar um conjunto de arquiteturas de rede neural para a previsão do nível de gravidade, segundo a classificação de KL em OA. RESULTADOS: em seguida da configuração da CNN, iniciou-se o treinamento de máquina com as radiografias, em seguida realizados testes e por fim a integração, obtendo-se uma classificação expressa com as de taxas de sensibilidade de teste com a densidade de rede com joelho saudável, OA leve, moderado e grave. Após a classificação e análise das imagens na rede convolucional, foi gerado o comportamento do classificador sobre a precisão do algoritmo na exatidão do diagnóstico. O algoritmo apresentou acurácia de 0,85 (OA mínimo), 0,79 ( joelho saudável), 0,89 (OA moderado) e 0,98 (OA severo. Precisão de 0,67 (joelho saudável), 0,71 (OA mínimo), 0,86 (OA moderado) e 0,82 (OA severo). Sensibilidade de 0,65 (joelho saudável), 0,77 (OA mínimo), 0,79 (OA moderado) e 0,93 (OA severo). Especificidade de 0,86 (joelho saudável), 0,88 (OA mínimo), 0,94 (OA moderado) e 0,93 (OA severo). CONCLUSÃO: Nossos modelos de aprendizagem profunda propostos forneceram alta precisão e acurácia satisfatória para a detecção e classificação de osteoartrite leve ao severo do joelho em radiografias simples. Esses modelos podem ser usados como auxílio no diagnóstico clínico de radiografias de joelho e na orientação do tratamento em cada estágio da patologia para médicos, radiologistas e profissionais do movimento humano},
   author = {Daniel Moreira PACCA and Gustavo Constantino DE-CAMPOS and Alessandro Rozin ZORZI and Elinton Adami CHAIM and Jõao Batista DE-MIRANDA},
   doi = {10.36692/15n1-03},
   journal = {Revista CPAQV - Centro de Pesquisas Avançadas em Qualidade de Vida},
   title = {DESENVOLVIMENTO E APLICAÇÃO DE REDE NEURAL CONVOLUCIONAL PARA O DIAGNÓSTICO DE OSTEOARTRITE DE JOELHO},
   volume = {15},
   url = {https://revista.cpaqv.org/index.php/CPAQV/article/view/1079},
   year = {2022},
}
@article{Mohammed2023,
   abstract = {One of the most common and challenging medical conditions to deal with in old-aged people is the occurrence of knee osteoarthritis (KOA). Manual diagnosis of this disease involves observing X-ray images of the knee area and classifying it under five grades using the Kellgren–Lawrence (KL) system. This requires the physician’s expertise, suitable experience, and a lot of time, and even after that the diagnosis can be prone to errors. Therefore, researchers in the ML/DL domain have employed the capabilities of deep neural network (DNN) models to identify and classify KOA images in an automated, faster, and accurate manner. To this end, we propose the application of six pretrained DNN models, namely, VGG16, VGG19, ResNet101, MobileNetV2, InceptionResNetV2, and DenseNet121 for KOA diagnosis using images obtained from the Osteoarthritis Initiative (OAI) dataset. More specifically, we perform two types of classification, namely, a binary classification, which detects the presence or absence of KOA and secondly, classifying the severity of KOA in a three-class classification. For a comparative analysis, we experiment on three datasets (Dataset I, Dataset II, and Dataset III) with five, two, and three classes of KOA images, respectively. We achieved maximum classification accuracies of 69%, 83%, and 89%, respectively, with the ResNet101 DNN model. Our results show an improved performance from the existing work in the literature.},
   author = {Abdul Sami Mohammed and Ahmed Abul Hasanaath and Ghazanfar Latif and Abul Bashar},
   doi = {10.3390/diagnostics13081380},
   issn = {20754418},
   issue = {8},
   journal = {Diagnostics},
   title = {Knee Osteoarthritis Detection and Severity Classification Using Residual Neural Networks on Preprocessed X-ray Images},
   volume = {13},
   year = {2023},
}
@misc{who2023,
  author       = {World Health Organization},
  title        = {Osteoarthritis},
  year         = {2023},
  howpublished = {\url{https://www.who.int/news-room/fact-sheets/detail/osteoarthritis}},
  note         = {Acessado em: 15 de agosto de 2024}
}
@article{Alshamrani2023,
   abstract = {Knee osteoarthritis is a challenging problem affecting many adults around the world. There are currently no medications that cure knee osteoarthritis. The only way to control the progression of knee osteoarthritis is early detection. Currently, X-ray imaging is a central technique used for the prediction of osteoarthritis. However, the manual X-ray technique is prone to errors due to the lack of expertise of radiologists. Recent studies have described the use of automated systems based on machine learning for the effective prediction of osteoarthritis from X-ray images. However, most of these techniques still need to achieve higher predictive accuracy to detect osteoarthritis at an early stage. This paper suggests a method with higher predictive accuracy that can be employed in the real world for the early detection of knee osteoarthritis. In this paper, we suggest the use of transfer learning models based on sequential convolutional neural networks (CNNs), Visual Geometry Group 16 (VGG-16), and Residual Neural Network 50 (ResNet-50) for the early detection of osteoarthritis from knee X-ray images. In our analysis, we found that all the suggested models achieved a higher level of predictive accuracy, greater than 90%, in detecting osteoarthritis. However, the best-performing model was the pretrained VGG-16 model, which achieved a training accuracy of 99% and a testing accuracy of 92%.},
   author = {Hassan A. Alshamrani and Mamoon Rashid and Sultan S. Alshamrani and Ali H.D. Alshehri},
   doi = {10.3390/healthcare11091206},
   issn = {22279032},
   issue = {9},
   journal = {Healthcare (Switzerland)},
   title = {Osteo-NeT: An Automated System for Predicting Knee Osteoarthritis from X-ray Images Using Transfer-Learning-Based Neural Networks Approach},
   volume = {11},
   year = {2023},
}
@article{KELLGREN1957,
   author = {J. H. KELLGREN and J. S. LAWRENCE},
   doi = {10.1136/ard.16.4.494},
   issn = {00034967},
   issue = {4},
   journal = {Annals of the rheumatic diseases},
   title = {Radiological assessment of osteo-arthrosis.},
   volume = {16},
   year = {1957},
}
@misc{Kanamoto2020,
   abstract = {Osteoarthritis (OA) is a degenerative joint disease that causes joint dysfunction due to pain and restricted range of motion and it causes lowering of physical activity and the quality of life and results in a significant effect on the global burden of disease. Early diagnosis of OA in the knee joint is critical for effective treatment before facing severe irreversible pathology, and to develop new OA treatment techniques. Discussion on defining early OA has become more globally active. In this review, two proposals of the definition of early knee osteoarthritis, the Italian Rheumatology Association International and the First International Early OA Workshop proposed are introduced. These definitions of early OA have been proposed with a combination of “symptoms” of joint pain, “signs” such as joint stiffness, tenderness, or risk factors, and diagnostic imaging such as radiographs. Consensus on a more detailed classification and validation of the proposed definition of early OA are necessary to develop new treatment methods to suppress or prevent symptoms at an early stage before any progressive and irreversible change. Diagnostic imaging techniques such as MRI and ultrasound are superior compared to X-ray for visualization of soft tissues of joints and quantification of physical activity will become a key for the evaluation and development of new treatment strategies for early stages of osteoarthritis in the near future due to the fact that the structural and qualitative abnormalities of joints directly affect physical activity, and improvement of that will be the primary outcome of treatment for osteoarthritis in society.},
   author = {Takashi Kanamoto and Tatsuo Mae and Teruki Yokoyama and Hiroyuki Tanaka and Kosuke Ebina and Ken Nakata},
   doi = {10.21037/aoj.2019.09.02},
   issn = {24156809},
   issue = {JANUARY},
   journal = {Annals of Joint},
   title = {Significance and definition of early knee osteoarthritis},
   volume = {5},
   year = {2020},
}
@article{Saraev2020,
   abstract = {Background. Current evidence based research data lead to reassessment of traditional approaches for treatment of patients with bone and joint disorders especially knee osteoarthritis (OA). The purpose of the study was to review randomized clinical trials (RCT) and meta-analyses of RCT as well as recent guidelines of professional societies for application of arthroscopic lavage, debridement and meniscectomy in knee OA. Materials and Methods. Databases PubMed, e-LIBRARY, EMBASE (Ovid), Cochrane Central Register of Controlled Trials (CENTRAL) were searched for the period from 2000 till 2019. From 138 heats irrelevant and poor quality studies were excluded. In total there were 1614 patients aged 48,9–62,8 in RCT and 20 770 patients aged 42–62,4 in meta-analyses of RCT. Results. Both arthroscopic lavage and debridement do not lead to significant pain relief as well as functional improvement in long term therefore are not recommended. Nonsurgical treatment should be the first line strategy in patients with early and moderate knee OA even with degenerative meniscal tears irrespective of mechanical symptoms like painful locking, catching or sudden giving way. Arthroscopy might be performed only if complex non-surgical treatment including non-steroidal anti-inflammatory drugs, structured exercises program and intra-articular injections failed after 3 months in patients without ‘bone on bone’ cartilage erosions and frontal malalignment or if the knee is mechanically locked due to bucked handle type meniscus tear or loose body. Conclusion. Evidence based medicine approach let us to conclude that arthroscopy in knee OA is non-efficient and rarely indicated therefore if proper non-surgical treatment is failed around the knee osteotomies and partial or total arthroplasty should be considered.},
   author = {A. V. Saraev and T. A. Kulyaba and M. Sh. Rasulov and N. N. Kornilov},
   doi = {10.21823/2311-2905-2020-26-4-150-162},
   issn = {2311-2905},
   issue = {4},
   journal = {Traumatology and Orthopedics of Russia},
   title = {Arthroscopy for Knee Osteoarthritis in the XXI Century: a Systematic Review of Current High Quality Researches and Guidelines of Professional Societies},
   volume = {26},
   year = {2020},
}
@article{Tariq2023,
   abstract = {Knee osteoarthritis is a common form of arthritis, a chronic and progressive disease recognized by joint space narrowing, osteophyte formation, sclerosis, and bone deformity that can be observed using radiographs. Radiography is regarded as the gold standard and is the cheapest and most readily available modality. X-ray images are graded using Kellgren and Lawrence's (KL) grading scheme according to the order of severity of osteoarthritis from normal to severe. Early detection can help early treatment and hence slows down knee osteoarthritis degeneration. Unfortunately, most of the existing approaches either merge or exclude perplexing grades to improve the performance of their models. This study aims to automatically detect and classify knee osteoarthritis according to the KL grading system for radiographs. We have proposed an automated deep learning-based ordinal classification approach for early diagnosis and grading knee osteoarthritis using a single posteroanterior standing knee x-ray image. An Osteoarthritis Initiative(OAI) based dataset of knee joint X-ray images is chosen for this study. The dataset was split into the training, testing, and validation set with a 7: 2: 1 ratio. We took advantage of transfer learning and fine-tuned ResNet-34, VGG-19, DenseNet 121, and DenseNet 161 and joined them in an ensemble to improve the model's overall performance. Our method has shown promising results by obtaining 98% overall accuracy and 0.99 Quadratic Weighted Kappa with a 95% confidence interval. Also, accuracy per KL grade is significantly improved. Furthermore, our methods outperform state-of-the-art automated methods.},
   author = {Tayyaba Tariq and Zobia Suhail and Zubair Nawaz},
   doi = {10.1109/ACCESS.2023.3276810},
   issn = {21693536},
   journal = {IEEE Access},
   title = {Knee Osteoarthritis Detection and Classification Using X-Rays},
   volume = {11},
   year = {2023},
}
@misc{Litjens2017,
   abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
   author = {Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A.W.M. van der Laak and Bram van Ginneken and Clara I. Sánchez},
   doi = {10.1016/j.media.2017.07.005},
   issn = {13618423},
   journal = {Medical Image Analysis},
   title = {A survey on deep learning in medical image analysis},
   volume = {42},
   year = {2017},
}

@misc{Shamshad2023,
   abstract = {Following unprecedented success on the natural language tasks, Transformers have been successfully applied to several computer vision problems, achieving state-of-the-art results and prompting researchers to reconsider the supremacy of convolutional neural networks (CNNs) as de facto operators. Capitalizing on these advances in computer vision, the medical imaging field has also witnessed growing interest for Transformers that can capture global context compared to CNNs with local receptive fields. Inspired from this transition, in this survey, we attempt to provide a comprehensive review of the applications of Transformers in medical imaging covering various aspects, ranging from recently proposed architectural designs to unsolved issues. Specifically, we survey the use of Transformers in medical image segmentation, detection, classification, restoration, synthesis, registration, clinical report generation, and other tasks. In particular, for each of these applications, we develop taxonomy, identify application-specific challenges as well as provide insights to solve them, and highlight recent trends. Further, we provide a critical discussion of the field's current state as a whole, including the identification of key challenges, open problems, and outlining promising future directions. We hope this survey will ignite further interest in the community and provide researchers with an up-to-date reference regarding applications of Transformer models in medical imaging. Finally, to cope with the rapid development in this field, we intend to regularly update the relevant latest papers and their open-source implementations at https://github.com/fahadshamshad/awesome-transformers-in-medical-imaging.},
   author = {Fahad Shamshad and Salman Khan and Syed Waqas Zamir and Muhammad Haris Khan and Munawar Hayat and Fahad Shahbaz Khan and Huazhu Fu},
   doi = {10.1016/j.media.2023.102802},
   issn = {13618423},
   journal = {Medical Image Analysis},
   title = {Transformers in medical imaging: A survey},
   volume = {88},
   year = {2023},
}

@misc{dataset-kaggle,
  author       = {Pingjun Chen},
  title        = {Knee Osteoarthritis Dataset with Severity Grading},
  year         = {2018},
  howpublished = {\url{https://www.kaggle.com/datasets/shashwatwork/knee-osteoarthritis-dataset-with-severity}},
  note         = {Acessado em: 29 de setembro de 2024}
}

@misc{oai,
  author       = {National Institutes of Health},
  title        = {Osteoarthritis Initiative},
  year         = {2024},
  howpublished = {\url{https://www.niams.nih.gov/grants-funding/funded-research/osteoarthritis-initiative}},
  note         = {Acessado em: 17 de julho de 2024}
}

@inproceedings{He2016,
   abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
   author = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
   doi = {10.1109/CVPR.2016.90},
   issn = {10636919},
   booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {Deep residual learning for image recognition},
   volume = {2016-December},
   year = {2016},
}

@inproceedings{Simonyan2015,
   abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
   author = {Karen Simonyan and Andrew Zisserman},
   booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
   title = {Very deep convolutional networks for large-scale image recognition},
   year = {2015},
}

@inproceedings{Huang2017,
   abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L2+1) direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
   author = {Gao Huang and Zhuang Liu and Laurens Van Der Maaten and Kilian Q. Weinberger},
   doi = {10.1109/CVPR.2017.243},
   booktitle = {Proceedings - 30th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017},
   title = {Densely connected convolutional networks},
   volume = {2017-January},
   year = {2017},
}

@inproceedings{Szegedy2016,
   abstract = {Convolutional networks are at the core of most state of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21:2% top-1 and 5:6% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3:5% top-5 error and 17:3% top-1 error on the validation set and 3:6% top-5 error on the official test set.},
   author = {Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jon Shlens and Zbigniew Wojna},
   doi = {10.1109/CVPR.2016.308},
   issn = {10636919},
   booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
   title = {Rethinking the Inception Architecture for Computer Vision},
   volume = {2016-December},
   year = {2016},
}

@inproceedings{Dosovitskiy2021,
   abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
   author = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
   booktitle = {ICLR 2021 - 9th International Conference on Learning Representations},
   title = {AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE},
   year = {2021},
}

@inproceedings{Touvron2021,
   abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. These high-performing vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption. In this work, we produce competitive convolution-free transformers trained on ImageNet only using a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop) on ImageNet with no external data. We also introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention, typically from a convnet teacher. The learned transformers are competitive (85.2% top-1 acc.) with the state of the art on ImageNet, and similarly when transferred to other tasks. We will share our code and models.},
   author = {Hugo Touvron and Matthieu Cord and Matthijs Douze and Francisco Massa and Alexandre Sablayrolles and Herve Jegou},
   booktitle = {Proceedings of Machine Learning Research},
   title = {Training data-efficient image transformers and distillation through attention},
   volume = {139},
   year = {2021},
}

@inproceedings{Liu2021,
   abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer.},
   author = {Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
   doi = {10.1109/ICCV48922.2021.00986},
   issn = {15505499},
   booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
   title = {Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
   year = {2021},
}

@article{Park2022,
   abstract = {Developing a robust algorithm to diagnose and quantify the severity of the novel coronavirus disease 2019 (COVID-19) using Chest X-ray (CXR) requires a large number of well-curated COVID-19 datasets, which is difficult to collect under the global COVID-19 pandemic. On the other hand, CXR data with other findings are abundant. This situation is ideally suited for the Vision Transformer (ViT) architecture, where a lot of unlabeled data can be used through structural modeling by the self-attention mechanism. However, the use of existing ViT may not be optimal, as the feature embedding by direct patch flattening or ResNet backbone in the standard ViT is not intended for CXR. To address this problem, here we propose a novel Multi-task ViT that leverages low-level CXR feature corpus obtained from a backbone network that extracts common CXR findings. Specifically, the backbone network is first trained with large public datasets to detect common abnormal findings such as consolidation, opacity, edema, etc. Then, the embedded features from the backbone network are used as corpora for a versatile Transformer model for both the diagnosis and the severity quantification of COVID-19. We evaluate our model on various external test datasets from totally different institutions to evaluate the generalization capability. The experimental results confirm that our model can achieve state-of-the-art performance in both diagnosis and severity quantification tasks with outstanding generalization capability, which are sine qua non of widespread deployment.},
   author = {Sangjoon Park and Gwanghyun Kim and Yujin Oh and Joon Beom Seo and Sang Min Lee and Jin Hwan Kim and Sungjun Moon and Jae Kwang Lim and Jong Chul Ye},
   doi = {10.1016/j.media.2021.102299},
   issn = {13618423},
   journal = {Medical Image Analysis},
   title = {Multi-task vision transformer using low-level chest X-ray feature corpus for COVID-19 diagnosis and severity quantification},
   volume = {75},
   year = {2022},
}

@article{Wang2021,
   abstract = {Osteoarthritis (OA) is the most common form of arthritis. According to the evidence presented on both sides of the knee bones, radiologists assess the severity of OA based on the Kellgren-Lawrence (KL) grading system. Recently, computer-aided methods are proposed to improve the efficiency of OA diagnosis. However, the human interventions required by previous semiautomatic segmentation methods limit the application on large-scale datasets. Moreover, well-known CNN architectures applied to the OA severity assessment do not explore the relations between different local regions. In this work, by integrating the object detection model, YOLO, with the visual transformer into the diagnosis procedure, we reduce human intervention and provide an end-to-end approach to automatic osteoarthritis diagnosis. Our approach correctly segments 95.57% of data at the expense of training on 200 annotated images on a large dataset that contains more than 4500 samples. Furthermore, our classification result improves the accuracy by 2.5% compared to the traditional CNN architectures.},
   author = {Yifan Wang and Xianan Wang and Tianning Gao and Le Du and Wei Liu},
   doi = {10.1155/2021/5586529},
   issn = {20402309},
   journal = {Journal of Healthcare Engineering},
   title = {An Automatic Knee Osteoarthritis Diagnosis Method Based on Deep Learning: Data from the Osteoarthritis Initiative},
   volume = {2021},
   year = {2021},
}

@article{Selvaraju2016,
   abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multimodal inputs (e.g. VQA) or reinforcement learning, without any architectural changes or re-training. We combine GradCAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on weakly-supervised localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, our visualizations show that even non-attention based models can localize inputs. Finally, we conduct human studies to measure if GradCAM explanations help users establish trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one. Our code is available at https://github.com/ramprs/grad-cam. A demo and a video of the demo can be found at http://gradcam.cloudcv.org and youtu.be/COjUB9Izk6E.},
   author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
   issn = {00418781},
   journal = {Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization},
   title = {Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization},
   volume = {17},
   year = {2016},
}
